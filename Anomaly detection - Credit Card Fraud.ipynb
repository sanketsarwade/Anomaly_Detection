{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d3fa2a",
   "metadata": {},
   "source": [
    "# Anomaly Detection:\n",
    "\n",
    "Anomaly detection is the process of identifying unusual patterns or outliers in data. The patterns that deviate significantly from the expected behavior are considered anomalies. Anomaly detection can be performed on various types of data such as time-series data, images, text, and numerical data.\n",
    "\n",
    "Anomalies can be caused by various factors such as fraud, errors, or system failures. Anomaly detection can help in identifying such unusual patterns, which can be further analyzed to understand the cause and take corrective actions. Anomaly detection can be useful in various domains such as finance, healthcare, and security."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e8949",
   "metadata": {},
   "source": [
    "We will use the Credit Card Fraud Detection dataset from Kaggle, which contains transactions made by credit cards in September 2013 by European cardholders. The dataset contains 284,807 transactions, out of which 492 are frauds. The dataset is highly imbalanced, with fraud transactions accounting for only 0.17% of the total transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1f536",
   "metadata": {},
   "source": [
    "You can download the credit card fraud dataset used in the project from the Kaggle website. Here is the link to the dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270dd71",
   "metadata": {},
   "source": [
    "Anomaly detection is essential in various fields and industries as it helps to identify unusual patterns or outliers in data that might indicate a problem or opportunity. Here are some reasons why anomaly detection is needed:\n",
    "\n",
    "* Fraud detection: Anomaly detection can help identify fraudulent activities, such as credit card fraud, insurance fraud, or healthcare fraud. By detecting anomalies, fraudsters can be caught and prevented from causing further damage.\n",
    "\n",
    "* Quality control: Anomaly detection can be used in manufacturing and production to identify defects or deviations from expected values. This can help in ensuring that the products meet the required quality standards.\n",
    "\n",
    "* Network security: Anomaly detection can help detect network intrusions, unauthorized access, or abnormal behavior on a network. This can help in preventing cyber attacks and protecting sensitive data.\n",
    "\n",
    "* Predictive maintenance: Anomaly detection can be used in maintenance and repair services to identify unusual patterns in machine data, indicating potential equipment failure. This can help in predicting and preventing breakdowns, reducing downtime and costs.\n",
    "\n",
    "* Health monitoring: Anomaly detection can be used in healthcare to identify abnormal patient behavior or health conditions. This can help in early diagnosis and treatment of illnesses, improving patient outcomes.\n",
    "\n",
    "In summary, anomaly detection is needed in many industries to identify and prevent problems, improve quality, and reduce costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d87069",
   "metadata": {},
   "source": [
    "Credit card fraud detection is one of the most important applications of anomaly detection. Anomaly detection can help in detecting fraudulent transactions that deviate from normal spending patterns of a cardholder. Here's how it works:\n",
    "\n",
    "* Historical data analysis: Anomaly detection algorithms are trained on historical data to learn normal spending patterns of a cardholder. This includes factors such as the amount spent, transaction location, time of day, and purchase category.\n",
    "\n",
    "* Real-time monitoring: As new transactions occur, anomaly detection algorithms compare the current transaction with the cardholder's historical spending patterns. If the current transaction deviates significantly from the learned patterns, it is flagged as a potential fraud.\n",
    "\n",
    "* Fraud scoring: Anomaly detection algorithms assign a fraud score to each flagged transaction based on how much it deviates from the learned patterns. Transactions with high fraud scores are further investigated by fraud analysts to confirm the fraud.\n",
    "\n",
    "* Adaptive learning: Anomaly detection algorithms continue to learn and adapt to new patterns as more data becomes available. This helps in improving the accuracy of fraud detection over time.\n",
    "\n",
    "In summary, anomaly detection can help in credit card fraud detection by learning normal spending patterns of a cardholder and flagging transactions that deviate significantly from these patterns. This can help in preventing fraudulent transactions, reducing losses for both cardholders and financial institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc93dbb",
   "metadata": {},
   "source": [
    "# Step 1: Data Preparation\n",
    "\n",
    "The first step is to prepare the data for anomaly detection. We will start by importing the necessary libraries and loading the dataset into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a31a731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (284807, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\SANKET\\OneDrive\\Desktop\\Anamoly detection\\creditcard.csv\")\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(\"Shape of the dataset:\", df.shape)\n",
    "\n",
    "# Check the first few rows of the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6f53c",
   "metadata": {},
   "source": [
    "The dataset contains 31 columns, including the Time, Amount, and Class columns. The Class column indicates whether a transaction is fraudulent or not, where 1 indicates fraud and 0 indicates non-fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3382b74c",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Before applying any anomaly detection algorithm, it is essential to preprocess the data to ensure that it is in a suitable format for the algorithm. Here are some steps that we can follow to preprocess the dataset:\n",
    "\n",
    "* Handling Missing Values\n",
    "Missing values can affect the performance of the anomaly detection algorithm. Therefore, it is essential to check whether there are any missing values in the dataset and take appropriate action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3c679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check if there are any missing values in the dataset\n",
    "print(df.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e63e33",
   "metadata": {},
   "source": [
    "The output shows that there are no missing values in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701f292",
   "metadata": {},
   "source": [
    "* Scaling the Data\n",
    "\n",
    "Anomaly detection algorithms can be sensitive to the scale of the data. Therefore, it is important to scale the data before applying the algorithm. We can use the StandardScaler class from the sklearn.preprocessing module to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf42203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.996583</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.996583</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.160686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.996562</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.140534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.996541</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0 -1.996583 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1 -1.996583  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2 -1.996562 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3 -1.996562 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4 -1.996541 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  0.239599  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.791461  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.237609  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.592941 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28    Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  0.244964      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724 -0.342475      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  1.160686      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  0.140534      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153 -0.073403      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the Amount column\n",
    "df['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "# Scale the Time column\n",
    "df['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))\n",
    "\n",
    "# Check the first few rows of the dataset after scaling\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8af07",
   "metadata": {},
   "source": [
    "# Anomaly Detection Algorithms\n",
    "There are various anomaly detection algorithms available. In this section, we will discuss some popular algorithms along with their implementation in Python.\n",
    "\n",
    "1. Isolation Forest\n",
    "Isolation Forest is a popular algorithm for anomaly detection that is based on the concept of decision trees. It works by creating random decision trees for the given data and isolating the anomalies by creating shorter paths for them.\n",
    "\n",
    "Let's implement the Isolation Forest algorithm on our credit card fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc404ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANKET\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:409: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 2849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Create the Isolation Forest object\n",
    "clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(0.01), max_features=1.0, random_state=42)\n",
    "\n",
    "# Fit the data and tag the outliers\n",
    "clf.fit(df)\n",
    "\n",
    "# Get the predictions\n",
    "y_pred = clf.predict(df)\n",
    "\n",
    "# Reshape the predictions to a 1D array\n",
    "y_pred = y_pred.reshape(-1,1)\n",
    "\n",
    "# Print the number of outliers\n",
    "print(\"Number of outliers:\", len(df[y_pred == -1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfb94d",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm has detected 2848 anomalies in the dataset.\n",
    "\n",
    "\n",
    "2. Local Outlier Factor\n",
    "Local Outlier Factor (LOF) is another popular algorithm for anomaly detection that is based on the concept of local density. It works by calculating the density of a data point relative to its neighbors and identifying points that have a much lower density than their neighbors as outliers.\n",
    "\n",
    "Let's implement the LOF algorithm on our credit card fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dba5045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers: 2849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Create the LOF object\n",
    "clf = LocalOutlierFactor(n_neighbors=20, contamination=float(0.01))\n",
    "\n",
    "# Fit the data and tag the outliers\n",
    "y_pred = clf.fit_predict(df)\n",
    "\n",
    "# Reshape the predictions to a 1D array\n",
    "y_pred = y_pred.reshape(-1,1)\n",
    "\n",
    "# Print the number of outliers\n",
    "print(\"Number of outliers:\", len(df[y_pred == -1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac0bb5b",
   "metadata": {},
   "source": [
    "The LOF algorithm has also detected 2848 anomalies in the dataset, which is the same as the Isolation Forest algorithm.\n",
    "\n",
    "3. One-class SVM\n",
    "One-class SVM is another popular algorithm for anomaly detection that is based on the concept of maximum margin hyperplanes. It works by creating a hyperplane that separates the normal data points from the anomalies and identifying points that lie on the wrong side of the hyperplane as anomalies.\n",
    "\n",
    "Let's implement the One-class SVM algorithm on our credit card fraud dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2524e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Define X and y\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598caba0",
   "metadata": {},
   "source": [
    "The One-class SVM algorithm has detected 492 anomalies in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1580df0",
   "metadata": {},
   "source": [
    "# Evaluation and Model Selection\n",
    "\n",
    "The code creates a list of classifiers to evaluate, which includes Logistic Regression and Decision Tree Classifier. Parameter grids are defined for each classifier. In the next step, the code loops over classifiers and parameter grids to find the best model. It uses GridSearchCV to search over the parameter grid for the best model.\n",
    "\n",
    "For each classifier, it fits the training data to the GridSearchCV object and prints the best parameters for that model. It then uses the trained model to predict on the test data and prints the classification report of the predicted results. The evaluation metrics such as precision, recall, and F1-score are printed for each class of the target variable, along with the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44369bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SANKET\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SANKET\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\SANKET\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\SANKET\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\SANKET\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan 0.9991523         nan 0.99917738        nan 0.9991824 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "{'C': 10, 'penalty': 'l2'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.88      0.63      0.74       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.94      0.82      0.87     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "DecisionTreeClassifier\n",
      "{'criterion': 'entropy', 'max_depth': 5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.91      0.81      0.86       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.90      0.93     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a list of classifiers to evaluate\n",
    "classifiers = [LogisticRegression(), DecisionTreeClassifier()]\n",
    "\n",
    "# Create parameter grids for each classifier\n",
    "lr_params = {'penalty': ['l1', 'l2'], 'C': [0.1, 1, 10]}\n",
    "dt_params = {'criterion': ['gini', 'entropy'], 'max_depth': [3, 5, 7]}\n",
    "rf_params = {'n_estimators': [100, 300, 500], 'max_depth': [3, 5, 7]}\n",
    "knn_params = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}\n",
    "param_grids = [lr_params, dt_params, rf_params, knn_params]\n",
    "\n",
    "# Loop over classifiers and parameter grids to find the best model\n",
    "for i, classifier in enumerate(classifiers):\n",
    "    clf = GridSearchCV(classifier, param_grids[i], cv=5)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(classifier.__class__.__name__)\n",
    "    print(clf.best_params_)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aba695",
   "metadata": {},
   "source": [
    "# Classification Metrics Evaluation\n",
    "In this code snippet, we are evaluating the performance of a classification model using various classification metrics.\n",
    "\n",
    "We are importing the following metrics from sklearn.metrics:\n",
    "\n",
    "* accuracy_score: computes the accuracy of the classifier by comparing the predicted labels to the true labels.\n",
    "* precision_score: computes the precision of the classifier by calculating the ratio of true positives to the sum of true positives and false positives.\n",
    "* recall_score: computes the recall of the classifier by calculating the ratio of true positives to the sum of true positives and false negatives.\n",
    "* f1_score: computes the F1 score, which is the harmonic mean of precision and recall.\n",
    "After predicting the labels using the model, we are computing the classification metrics using the accuracy_score, precision_score, recall_score, and f1_score functions. Finally, we are printing the results of these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bf49409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9995669627705019\n",
      "Precision: 0.9090909090909091\n",
      "Recall: 0.8088235294117647\n",
      "F1 Score: 0.8560311284046692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# evaluate the model's performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# print the classification metrics\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Precision: {prec}\")\n",
    "print(f\"Recall: {rec}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5311a4ec",
   "metadata": {},
   "source": [
    "The evaluation metrics provide information about the performance of a classification model. Here are the interpretations for the metrics obtained from the given code:\n",
    "\n",
    "* Accuracy: It is the ratio of the number of correctly predicted instances to the total number of instances. In this case, the accuracy is 0.9995, which means that the model predicted 99.95% of the test set correctly.\n",
    "\n",
    "* Precision: It is the ratio of the number of true positive predictions to the total number of positive predictions made by the model. In this case, the precision is 0.909, which means that out of all the positive predictions made by the model, only 90.9% of them are correct.\n",
    "\n",
    "* Recall: It is the ratio of the number of true positive predictions to the total number of actual positive instances in the test set. In this case, the recall is 0.809, which means that out of all the actual positive instances in the test set, the model predicted only 80.9% of them correctly.\n",
    "\n",
    "* F1 Score: It is the harmonic mean of precision and recall. In this case, the F1 score is 0.856, which means that the model has a good balance between precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfbfd2",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "In this credit card fraud project, we have analyzed a dataset containing credit card transactions and built a model to identify fraudulent transactions. We first performed exploratory data analysis to gain insights into the data and visualize the distributions of various features. We found that the data was highly imbalanced with a very small percentage of transactions being fraudulent.\n",
    "\n",
    "To build a model, we first preprocessed the data by scaling the numerical features and encoding the categorical features. We then split the data into training and testing sets and trained several classifiers using grid search to find the best hyperparameters. We evaluated the model's performance using classification metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Our final model, a logistic regression classifier, achieved high accuracy of 99.96% and a precision score of 0.91, indicating that the model is very good at identifying fraudulent transactions. The recall score of 0.81 shows that the model may still miss some fraudulent transactions, but overall it performs very well.\n",
    "\n",
    "In conclusion, the model we have built can help financial institutions detect fraudulent transactions and prevent financial loss. However, it is important to note that this is an ongoing battle as fraudsters are constantly evolving their tactics and the data is constantly changing. Therefore, continuous monitoring and improvement of the model is necessary to maintain its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd167272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
